#!/usr/bin/env python
"""
app/services/model_server.py
────────────────────────────────────────────────────────────────────────
Light-weight gRPC micro-service that keeps the (heavy) HF model
resident in RAM and streams tokens back to FastAPI callers.

• Proto  : proto/model.proto  (no package statement)
• Service: Generator.StreamGenerate   (unary request  → streamed chunks)

Start it once with:

    ./start_llm.sh
"""
from __future__ import annotations

import asyncio
import logging
import os
import threading
from pathlib import Path
from typing import Iterable, List

import grpc
from transformers import (
    GenerationConfig,
    StoppingCriteria,
    StoppingCriteriaList,
    TextIteratorStreamer,
)

# ─── gRPC stubs (generated by start.sh) ──────────────────────────────────
import model_pb2 as pb
import model_pb2_grpc as pbr          # type: ignore

# ─── One-time heavyweight import ─────────────────────────────────────────
from app.services.llm_loader import model, tokenizer           # noqa: E402

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("model_server")

# ------------------------------------------------------------------------
# Helper: generic “stop on these strings” criterion (works for any model)
# ------------------------------------------------------------------------
_DEFAULT_STOP_STRINGS = ["</s>", "</assistant>", "### End"]


class StopOnStrings(StoppingCriteria):
    """
    Abort generation once *any* of the given full **token sequences**
    has just appeared at the end of the output.
    """

    def __init__(self, stop_strings: Iterable[str]):
        super().__init__()
        self.seqs: List[List[int]] = [
            tokenizer.encode(s, add_special_tokens=False) for s in stop_strings
        ]

    def __call__(self, input_ids, scores, **kwargs) -> bool:  # noqa: D401
        for seq in self.seqs:
            if len(input_ids[0]) < len(seq):
                continue
            window = input_ids[0, -len(seq):].tolist()
            if window == seq:
                return True
        return False


# ──────────────────────────── Servicer ──────────────────────────────────
class GeneratorServicer(pbr.GeneratorServicer):                # type: ignore
    """Implements the bidirectional *StreamGenerate* RPC."""

    async def StreamGenerate(                                  # noqa: D401
        self,
        request: pb.GenerateRequest,
        context: grpc.aio.ServicerContext,
    ):
        # ───── build chat-template prompt ───────────────────────────────
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant.",
            },
            {
                "role": "user",
                "content": request.user_content,
            },
        ]

        input_ids = tokenizer.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,   # adds the "<assistant>" prefix
            return_tensors="pt",
        ).to(model.device)

        logger.info("⏩  Received user content (%d chars)", len(request.user_content))

        # ───── streamer converts token IDs → text chunks ────────────────
        streamer = TextIteratorStreamer(
            tokenizer,
            skip_prompt=True,
            skip_special_tokens=True,
            decode_kwargs={"skip_special_tokens": True},
        )

        # ───── generation parameters ───────────────────────────────────
        max_new     = request.max_new_tokens or int(os.getenv("MAX_TOKENS", "256"))
        temperature = request.temperature      or 0.7
        top_p       = request.top_p            or 0.9

        stop_strings = (
            os.getenv("STOP_STRINGS", "|".join(_DEFAULT_STOP_STRINGS)).split("|")
        )
        stopping = StoppingCriteriaList([StopOnStrings(stop_strings)])

        # ───── run blocking generate() in a background thread ───────────
        def _run_generation() -> None:
            model.generate(
                input_ids=input_ids,
                streamer=streamer,
                generation_config=GenerationConfig(
                    max_new_tokens=max_new,
                    do_sample=True,
                    temperature=temperature,
                    top_p=top_p,
                    eos_token_id=tokenizer.eos_token_id,
                    pad_token_id=tokenizer.eos_token_id,
                ),
                stopping_criteria=stopping,
            )

        threading.Thread(target=_run_generation, daemon=True).start()

        # ───── yield tokens as they appear ─────────────────────────────
        for token in streamer:
            yield pb.GenerateChunk(text=token)


# ──────────────────────────── Server loop ───────────────────────────────
async def serve() -> None:
    server = grpc.aio.server()
    pbr.add_GeneratorServicer_to_server(GeneratorServicer(), server)        # type: ignore
    listen_addr = "[::]:50051"
    server.add_insecure_port(listen_addr)
    logger.info("🚀  LLM gRPC server listening on %s", listen_addr)

    await server.start()
    await server.wait_for_termination()


if __name__ == "__main__":
    # Make sure repo-root is on PYTHONPATH when run directly
    root = Path(__file__).resolve().parents[2]
    import sys

    if str(root) not in sys.path:
        sys.path.insert(0, str(root))

    asyncio.run(serve())
