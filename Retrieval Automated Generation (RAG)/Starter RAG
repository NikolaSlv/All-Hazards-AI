# LLM-Controlled Simulation System

# --- 1. LLM SETUP (using Ollama or LM Studio) ---
# This step assumes you have installed and set up an LLM like LLaMA3 via Ollama:
# ollama run llama3

# --- 2. SIMULATION CONTROL SERVER ---
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import subprocess
import psycopg2
import logging

app = FastAPI()

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- 3. DATA MODEL ---
class SimulationRequest(BaseModel):
    condition: str

# --- 4. DATABASE FUNCTION ---
def fetch_data_from_db(condition: str):
    try:
        conn = psycopg2.connect(
            database="your_database",
            user="your_username",
            password="your_password",
            host="localhost",
            port="5432"
        )
        cur = conn.cursor()
        cur.execute("SELECT * FROM inputs WHERE condition = %s", (condition,))
        data = cur.fetchall()
        conn.close()
        logging.info("Data fetched successfully from database.")
        return data
    except Exception as e:
        logging.error(f"Error fetching data: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# --- 5. SIMULATION FUNCTION ---
def run_simulation(data):
    try:
        # Example: Write data to input file
        with open("input_data.txt", "w") as f:
            for row in data:
                f.write(",".join(map(str, row)) + "\n")

        # Launch external simulation program
        process = subprocess.Popen(["/path/to/your_simulation_app.exe", "input_data.txt"])
        logging.info("Simulation started.")
        return {"message": "Simulation started", "pid": process.pid}
    except Exception as e:
        logging.error(f"Simulation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# --- 6. API ENDPOINT ---
@app.post("/run-simulation")
def api_run_simulation(req: SimulationRequest):
    data = fetch_data_from_db(req.condition)
    return run_simulation(data)

# --- 7. LANGCHAIN AGENT SETUP (Run Separately in a Python Agent Script) ---
# from langchain.agents import initialize_agent, Tool
# from langchain.llms import Ollama
#
# def send_simulation_request(condition):
#     import requests
#     return requests.post("http://localhost:8000/run-simulation", json={"condition": condition}).json()
#
# tools = [
#     Tool(
#         name="Run Simulation",
#         func=send_simulation_request,
#         description="Run a simulation with a specified condition."
#     )
# ]
#
# llm = Ollama(model="llama3")
# agent = initialize_agent(tools, llm, agent_type="zero-shot-react-description")
# agent.run("Run a simulation where condition equals 'X'.")

# To run:
# uvicorn this_file_name:app --reload
# Then launch the agent script in a separate process
